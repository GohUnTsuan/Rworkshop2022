---
title: "Lab 1: 基于软件的数据抓取"
author: "孙宇飞（清华大学政治学系博士生）"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
    # css: "style_ui.css"
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(drhur)
library(learnr)
library(tidyverse)
library(lubridate) 
```


# 与爬虫初会

## 大纲

- 初窥门径：爬虫基础知识

- 利刃出鞘：如何爬取数据？

- 知己知彼：当前主要的反爬手段

- 初学乍练：如何爬取微信公众号数据（1）

## 初窥门径：爬虫基础知识

### 什么是爬虫

在广袤的互联网中，有这样一种"爬虫生物"，穿梭于万维网中，将承载信息的网页吞食，然后交由搜索引擎进行转化，吸收，并最终"孵化"出结构化的数据，供人快速查找，展示。

这种"生物"，其名曰"网络蜘蛛"（又被称为网页蜘蛛，网络机器人）。网络蜘蛛虽以数据为食，但是数据的生产者-网站，也需要借助爬虫的帮助，将网页提交给搜索引擎。

```{r out.width = "60%", echo = FALSE}
knitr::include_graphics("http://www.chipscoco.com/zb_users/upload/2021/02/202102051612492579435384.jpg")
```

### 为什么要爬虫

- 社会科学研究需要的数据更加多元

- 但数据源拒绝给我们结构化的数据查询方式或API

### 爬虫的历史

**早期爬虫**。斯坦福的几位同学完成的抓取，当时的互联网基本都是完全开放的，人类流量是主流；

**分布式爬虫**。但是爬虫面对新的问题是数据量越来越大，传统爬虫已经解决不了把数据都抓全的问题，需要更多的爬虫，于是调度问题就出现了；

**暗网（英语：Darknet或Dark Web）爬虫**。此时面对新的问题是数据之间的link越来越少，比如淘宝，点评这类数据，彼此link很少，那么抓全这些数据就很难；还有一些数据是需要提交查询词才能获取，比如机票查询，那么需要寻找一些手段“发现”更多，更完整的不是明面上的数据。

**智能爬虫**，这主要是爬虫又开始面对新的问题：社交网络数据的抓取。

社交网络对爬虫带来的新的挑战包括

- 有一条账号护城河

我们通常称UGC（User Generated Content）指用户原创内容。

每个人都通过账号来标识身份，提交数据，这样一来社交网络就可以通过封账号来提高数据抓取的难度，通过账号来发现非人类流量。
之前没有账号只能通过cookie和ip。cookie又是易变，易挥发的，很难长期标识一个用户。

- 网络走向封闭

新浪微博在2012年以前都是基本不封的，但是很快，越来越多的站点都开始防止竞争对手，防止爬虫来抓取，数据逐渐走向封闭，越来越多的人难以获得数据。甚至都出现了专业的爬虫公司。

- 反爬手段，封杀手法千差万别

写一个通用的框架抓取成百上千万的网站已经成为历史，或者说已经是一个技术相对成熟的工作，也就是已经有相对成熟的框架来”盗“成百上千的墓，但是极个别的墓则需要特殊手段了，目前市场上比较难以抓取的数据包括，

  - 微信公众号
  
  - 微博
  
  - Facebook 
  
  - Ins
  
  - 淘宝

当面对以上三个挑战的时候，就需要智能爬虫。智能爬虫是让爬虫的行为尽可能模仿人类行为，让反爬策略失效。

### 爬虫的典型案例：搜索引擎

搜索引擎是Web时代用户使用互联网的入口和指南。

网络爬虫是搜索引擎系统中十分重要的组成部分，它负责从互联网中搜集网页，采集信息，这些网页信息用于建立索引从而为搜索引擎提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。

```{r out.width = "60%", echo = FALSE}
knitr::include_graphics("https://piaosanlang.gitbooks.io/spiders/content/photos/01-engine.png")
```

### 搜索引擎工作原理和爬虫的基本分类

```{r out.width = "60%", echo = FALSE}
knitr::include_graphics("https://piaosanlang.gitbooks.io/spiders/content/photos/01-engine.png")
```

第一步：抓取网页（爬虫）

搜索引擎是通过一种特定规律的软件跟踪网页的链接，从一个链接爬到另外一个链接，像蜘蛛在蜘蛛网上爬行一样，所以被称为“蜘蛛”也被称为“机器人”。搜索引擎蜘蛛的爬行是被输入了一定的规则的，它需要遵从一些命令或文件的内容。

Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。

如淘宝的[爬虫协议](https://www.taobao.com/robots.txt)

第二步：数据存储

搜索引擎是通过蜘蛛跟踪链接爬行到网页，并将爬行的数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。
第三步：预处理
搜索引擎将蜘蛛抓取回来的页面，进行各种步骤的预处理。

⒈提取文字 ⒉中文分词 ⒊去停止词 ⒋消除噪音（搜索引擎需要识别并消除这些噪声，比如版权声明文字、导航条、广告等……） 5.正向索引 6.倒排索引 7.链接关系计算 8.特殊文件处理

除了HTML文件外，搜索引擎通常还能抓取和索引以文字为基础的多种文件类型，如 PDF、Word、WPS、XLS、PPT、TXT 文件等。我们在搜索结果中也经常会看到这些文件类型。

第四步：排名，提供检索服务

但是，这些通用性搜索引擎也存在着一定的局限性，如：

(1)不同领域、不同背景的用户往往具有不同的检索目的和需求，通用搜索引擎所返回的结果包含大量用户不关心的网页。

(2)通用搜索引擎的目标是尽可能大的网络覆盖率，有限的搜索引擎服务器资源与无限的网络数据资源之间的矛盾将进一步加深。

(3)万维网数据形式的丰富和网络技术的不断发展，图片、数据库、音频、视频多媒体等不同数据大量出现，通用搜索引擎往往对这些信息含量密集且具有一定结构的数据无能为力，不能很好地发现和获取。

(4)通用搜索引擎大多提供基于关键字的检索，难以支持根据语义信息提出的查询。

为了解决上述问题，定向抓取相关网页资源的聚焦爬虫应运而生。

聚焦爬虫是一个自动下载网页的程序，它根据既定的抓取目标，有选择的访问万维网上的网页与相关的链接，获取所需要的信息。

与通用爬虫(general purpose web crawler)不同，聚焦爬虫并不追求大的覆盖，而将目标定为抓取与某一特定主题内容相关的网页，为面向主题的用户查询准备数据资源。

聚焦爬虫工作原理以及关键技术概述

网络爬虫是一个自动提取网页的程序，它为搜索引擎从万维网上下载网页，是搜索引擎的重要组成。传统爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列,直到满足系统的一定停止条件。聚焦爬虫的工作流程较为复杂，需要根据一定的网页分析算法过滤与主题无关的链接，保留有用的链接并将其放入等待抓取的URL队列。然后，它将根据一定的搜索策略从队列中选择下一步要抓取的网页URL，并重复上述过程，直到达到系统的某一条件时停止。另外，所有被爬虫抓取的网页将会被系统存贮，进行一定的分析、过滤，并建立索引，以便之后的查询和检索；对于聚焦爬虫来说，这一过程所得到的分析结果还可能对以后的抓取过程给出反馈和指导。

相对于通用网络爬虫，聚焦爬虫还需要解决三个主要问题：

(1) 对抓取目标的描述或定义

(2) 对网页或数据的分析与过滤；

(3) 对URL的搜索策略。

抓取目标的描述和定义是决定网页分析算法与URL搜索策略如何制订的基础。

而网页分析算法和候选URL排序算法是决定搜索引擎所提供的服务形式和爬虫网页抓取行为的关键所在。这两个部分的算法又是紧密相关的。

### 爬虫的基本原理

> “网站是把个人计算机连上网络的国晨，爬虫就是通过网络到别人计算机下载数据的过程”

爬虫是模拟用户在浏览器或者某个应用上的操作，把操作的过程、实现自动化的程序。
当我们在浏览器中输入一个url后回车，后台会发生什么？

简单来说这段过程发生了以下四个步骤：

- 查找域名对应的IP地址。

- 向IP对应的服务器发送请求。

- 服务器响应请求，发回网页内容。

- 浏览器解析网页内容。

```{r out.width = "60%", echo = FALSE}
knitr::include_graphics("https://piaosanlang.gitbooks.io/spiders/content/photos/01-webdns.jpg")
```

> 为什么需要DNS服务器？

### 浏览器是如何发送和接收数据？

HTTP略

## 利刃出鞘：如何爬取数据？

- 基于封装软件的数据爬取：以“八爪鱼”为例

- 基于编程语言的数据爬取：Python和R的混合使用

### 基于封装软件的数据爬取：以“八爪鱼”为例

https://www.bazhuayu.com/tutorial8/81khdjm

## 知己知彼：当前主要的反爬手段

## 初学乍练：如何爬取微信公众号数据（1）

### 关关难过关关过：项目解析和难点突破

关卡1：数据封闭，全部在较为封闭的微信客户端中封存

难点突破：

App时代的反抗：能否找到Web接口？

能否用网页微信？不行！（账户类型要求，无法读取公众号或评论）

能否在浏览器中打开？也许可以！

关卡2：如何获取稳定链接？

关卡3：如何批量获取文章链接？

难点突破：

> "自己人！自己人"

> "混在老百姓队伍里面，才是安全的"

使用微信公众号平台的引文功能进行获取

## 手把手1：使用八爪鱼完成上述内容

## 动动手1：根据分组完成两个地级市的本地宝和官方发布公众号的爬取

